+++
date = '2025-10-18T22:12:20+08:00'
draft = true
title = '语言模型发展论文集'
+++

## NNLM

2003 《A Neural Probabilistic Language Model》

统计语言建模目的是学习 **单词序列的联合概率函数**（叫大模型解释）。由于维度灾难：测试集的单词序列可能出现训练集中从未出现的单词序列。传统n-gram词袋模型成功实现 泛化能力 通过马尔可夫链。文章提出学习词语的 分布式表示 ，该模型会同时学习：（1）每个词的**分布式表示**（2）一个以这些表示为基础的词**序列概率函数**。

<img src="2003.png" alt="2003" style="zoom:50%;" />

### 任务与目标

训练一个语言模型，根据上下文 预测下一个词。

### 模型简化设定

从图中底到顶：

- 当前遇到的训练句子是 "a cat sat on a mat"
- **上下文长度 (n-1)**: 只看前面 **2** 个词（主观），也就是最底下绿色方块只有俩，w_t-2, w_t-1。
- **词向量维度 (m)**: 把每个词表示成一个 **3** 维的向量（主观），从绿色方块变成3个红色点。
- **隐藏层大小 (h)**: 我们的隐藏层有 **2** 个神经元（主观），到中间红点这里会乘以一个矩阵，矩阵的规模和2个神经元有关。
- **词汇表 (V)**: 我们的整个语言只有 6 个词(客观)：`{a, cat, mat, on, sat, the}`。它们的索引分别是 `{0, 1, 2, 3, 4, 5}`。

### 正向传播

从图中底部流向顶部：

- **输入与查表**：从最底绿点到倒二红点，w是词，绿块是w的索引, 倒二行红点是w的词向量 C(w)。   

  训练句子 "a cat sat on a mat" 截取出(a, cat, sat), (cat, sat, on)..., 第一条为(a, cat, sat)。     

  输入: 上下文是 "a cat"。它们的索引分别是 `0` 和 `1`。      

  查表 (C): 我们有一个 `6x3` 的词向量矩阵 `C` (6个词，每个3维)，它刚被随机初始化：    

  | 词 (索引) | 词向量 `C(word)`  |
  | :-------- | :---------------- |
  | **a (0)** | `[0.1, 0.2, 0.3]` |
  | cat (1)   | `[0.4, 0.5, 0.6]` |
  | mat (2)   | `[0.7, 0.8, 0.9]` |
  | on (3)    | `[1.0, 1.1, 1.2]` |
  | sat (4)   | `[1.3, 1.4, 1.5]` |
  | the (5)   | `[1.6, 1.7, 1.8]` |
  
  我们从这个表里查出输入词的向量：
  
  - `C(a)` = `[0.1, 0.2, 0.3]` C(w_t-2)
  - `C(cat )` = `[0.4, 0.5, 0.6]` C(w_t-1)

- **拼接输入向量**

  把上一步得到的两个词向量拼接成一个大向量 `x`，形成 tanh 下的长红点。   
  `x = [ C(sat), C(on) ] = [1.3, 1.4, 1.5, 1.0, 1.1, 1.2] `  
  这个 `x` 的维度是 `(n-1) * m = 2 * 3 = 6`（看了前两个词，每个词向量长度为3）。   

- **计算隐藏层** (tanh)   

  现在信息流向了中间的 `tanh` 隐藏层。计算公式是 `tanh(d + Hx)`。      

  - `x`: 就是我们上一步得到的 `[1.3, 1.4, 1.5, 1.0, 1.1, 1.2]`。   
  - `H`: 是一个权重矩阵，它的维度是 `h x (n-1)m` = `2 x 6`，6 对应于大向量`x`长度为6，2是人为设置的，即神经元个数（复杂的矩阵能解决复杂的问题）。   
  - `d`: 是隐藏层的偏置向量。它的维度是 `h = 2`，对应神经元个数。    

  初始时刻， `H` 和 `d` 也被随机初始化了：   
  `H = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], [0.7, 0.8, 0.9, 1.0, 1.1, 1.2]]`    

  `d = [0.1, 0.2]`        

  现在计算 `d + Hx`:    
  `H x^T = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]·[1.3, 1.4, 1.5, 1.0, 1.1, 1.2]^T, [0.7, 0.8, 0.9, 1.0, 1.1, 1.2]·[1.3, 1.4, 1.5, 1.0, 1.1, 1.2]^T]` (矩阵乘法) = `[4.81, 11.23] `  
  `d + Hx = [0.1, 0.2] + [4.81, 11.23] = [4.91, 11.43]`   

  然后应用 `tanh` 函数，得到隐藏层的输出 `h_out`: `h_out = tanh([4.91, 11.43]) ≈ [0.999, 1.0]`

- **计算输出层**   

  现在信息将形成最顶层的红点，即一个6维向量，表示所**有词的出现概率**。计算公式是    

  ​                        `y = b + Wx + U * h_out` = `b + Wx + U * tanh(d + Hx)`。    
  
  - `b`: 输出层的偏置，维度是 `|V| = 6` （因为词的个数是6）。    
  - `W`: 从输入层到输出层的直连权重，维度是 `|V| x (n-1)m` = `6 x 6`，（W虽然乘的是x，但其实相当于W可选，可设置为0）。
  - `U`: 从隐藏层到输出层的权重，维度是 `|V| x h` = `6 x 2`。
  - （PS：说白了 Wx 是 输入-输出，U * h_out 是 输入-隐藏-输出，这里加和作为一个"并行融合"）    
  
  假设它们也被随机初始化了：   
  `U * h_out`: `6x2` 矩阵乘以 `2x1` 向量 `[0.999, 1.0]`，得到一个 `6x1` 的向量。   
  `W * x`: `6x6` 矩阵乘以 `6x1` 向量 `[0.3, ..., 0.2]`，得到一个 `6x1` 的向量。
  
  最后把这三者加起来，我们得到一个 `6x1` 的向量 `y`，它代表了词汇表中**每个词的“原始分数”（logits）**：   
  `y = [score_a, score_cat, score_mat, score_on, score_sat, score_the] `  
  假设算出来 `y = [0.5, 0.2, 0.9, -1.2, 0.1, 0.8]`


- **Softmax**    

  `Softmax` 函数把这些原始分数转换成和为1的概率分布。   
  `P(word) = e^(score_word) / ∑ e^(all_scores)`   

  `P("a") = e^0.5 / (e^0.5 + e^0.2 + ... + e^0.8) ≈ 0.17`   
  `P("cat") ≈ 0.12 `  
  `P("mat") ≈ 0.25 `  
  ... 等等，所有概率加起来等于1。

- **正向传播完成！** 

### 反向传播

我们的模型预测“mat”的概率最高，但正确答案是“sat”。现根据这个**错误**，从图的**顶部反向传播**回去，微调参数 (可能的参数：`C`, `H`, `d`, `U`, `b`, `W`)。

自己找个简单例子推反向传播，不展示了，损失函数计算损失，然后偏导为原理，优化器优化deta.... 注意的是，**最后会调整 C 从而实现词向量的训练**。

## CBOW与Skip-gram

《Efficient Estimation of Word Representations in Vector Space》

### 链接与要点

**必看链接**: [深入浅出：用中学数学理解 CBOW与Skip-gram](https://www.bilibili.com/video/BV1uF4m1P7HS/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

**要点**: 在理解NNLM的基础上，理解 多层softmax、CBOW、Skip-gram

### NNLM计算复杂度

*   **模型结构**: 输入层 -> 投影层 (查找词向量) -> 非线性隐藏层  -> 输出层 (Softmax)。
*   **计算复杂度**: `Q = N × D + N × D × H + H × V`
    *   `N × D`: 词向量查找。
    *   **`N × D × H`**: 从投影层到**隐藏层**的矩阵乘法。`N × D`是词向量拼接后的向量长度，而每个神经元里都有1个 `N × D` 长度的向量与之相乘变为一个值，共有`H`个神经元，所以是`H`次`(1, N×D)·(N×D, 1)`，即`(1, N×D)·(N×D, H)`，两个数值的乘法次数就有`N × D × H`次。
    *   **`H × V`**: 从隐藏层到**输出层**的矩阵乘法。`V`是词汇表大小，也是输出层的神经元个数，`(1, H)·(H, V)`得到`V`个值。

###  分层Softmax的NNLM

- 对原始NNLM的输出层的优化。

   *   **模型结构**: 输入层 -> 投影层 -> 非线性隐藏层  -> 输出层 (**分层Softmax**)。

   *   **计算复杂度**: `Q = N × D + N × D × H + H × log₂(V)`，**`H × log₂(V)为输出层的计算量被指数级优化。


- 分层Softmax

   *   **传统Softmax**：一个巨大的、`H × V` 大小的权重矩阵，`H`维向量跟表里的每一列（共V列）都做一次内积运算，完成 `H × V` 的计算。


   *   **分层Softmax**：建立了一棵**“二叉导航树”**。
       *   **树的叶子节点 **：词汇表中的 V 个单词，每个单词都作为一个叶子节点。
       *   **树的内部节点**：这些是路径上的“岔路口”，总共有 V-1 个。
       *   **关键点**：我们不再为V个单词存储`H`维向量，而是为 **V-1 个内部节点（岔路口）**各自分配一个`H`维的辅助向量。

   - 正向传播：

     - 回顾任务，已知前N个词，预测第N+1个词，构造的训练数据中知道第N+1个词是哪个词，假设为"cat"。

     - 在我们的二叉树中，从根节点到 "cat" 有一条唯一的路径，比如这条路径是 [左, 右, 左]，这个**路径的长度大约是 `log₂(V)`**。

     - 隐藏层输出的向量 `h` (维度为 H)，从根节点出发。第一个岔路口向左，计算“向左走”的概率，默认神经元输出的是向左的概率，`score₁ = h · v'_1` ，于是`P(向左 | Node 1) = σ(score₁)`；第二个岔路口 (Node 2) `score₂ = h · v'_2`，`P(向右 | Node 2) = 1 - σ(score₂)`； `score₃ = h · v'_3`， `P(向左 | Node 3) = σ(score₃)`；计算最终概率`P("cat") = P(向左|Node 1) × P(向右|Node 2) × P(向左|Node 3)`。

     - 叶子节点有`V`个，也就是词汇总数，`2^n = V`，所以**路径**的平均长度约为 `log₂(V)`；在路径上的**每一个节点上的计算量**为向量 `h` 和节点向量 `v'_j` 的点积 ，两个 H 维向量的点积，需要 H 次乘法 和 `H-1` 次加法。所以计算复杂度是 `O(H)`。**总计算量 = (路径上的节点数) × (每个节点的计算量)** ≈ log₂(V) × H。

     - 反向传播只调整路径上的节点，概率精度要求极高的任务上可能会略逊于标准Softmax。（我的经验主义想法：每次反向传播调整的节点变少，在数据量不够多的情况下效果没那么好，但是速度快了许多而且是自训练所以，牛掰。）



### CBOW 

连续词袋模型，革命性简化，但数据量上增大实现更好效果。

*   **模型结构**: 输入层 -> 投影层 (**查找并平均化上下文词向量**) -> 输出层 (分层Softmax)。
*   **计算复杂度**: `Q = N × D + D × log₂(V)`
    *   `N × D`: 将上下文词向量求平均的操作。
    *   `D × log₂(V)`: 直接用平均后的向量（维度为D）去通过分层Softmax做预测。
 *   **关键变化**:
     - **移除隐藏层**：`H × D` 的计算瓶颈消失
     - **输入变为平均向量**: 输入的维度从 `(N-1)×D` 降维到 `D`。

### Skip-gram

这是 Word2Vec 的第二个模型，思路与 CBOW 相反。

*   **模型结构**: 输入层 (单个词) -> 投影层 -> 输出层 (**分层Softmax**)。
*   **核心任务**: 根据一个**中心词**，来预测它周围的**上下文词**。
*   **计算复杂度**: `Q = C × (D + D × log₂(V))`
    *   `D`: 输入词向量查找。
    *   `D × log₂(V)`: 预测**一个**上下文单词的计算量。
    *   `C`: 平均每个中心词需要预测的上下文单词数量，其大小跟刚才N类似
    *   对于一个中心词，需要进行 `C` 次独立的正向、反向传播。

| 特性                 | **CBOW **                                                   | **Skip-gram**    |
| :------------------- | :---------------------------------------------------------- | :--------------- |
| **反向传播更新对象** | 上下文词向量                                                | 中心词向量       |
| **更新特点**         | 一次更新，梯度被**均摊**，`梯度 = “∂L/∂h  除以 上下文词数”` | 梯度**完整作用** |
| **训练速度**         | 较快                                                        | 较慢             |

---

### 总结

| 模型          | 隐藏层  | 输出层          | 任务             | 复杂度概览          | 计算复杂度                        |
| :------------ | :------ | :-------------- | :--------------- | :------------------ | --------------------------------- |
| **NNLM**      | **有 ** | **标准Softmax** | 预测下一个词     | **非常高**          | `N × D + N × D × H + H × V`       |
| **NNLM + HS** | **有**  | 分层Softmax     | 预测下一个词     | **高**              | `N × D + N × D × H + H × log₂(V)` |
| **CBOW**      | **无**  | 分层Softmax     | 上下文 -> 中心词 | **非常低**          | `N × D + D × log₂(V)`             |
| **Skip-gram** | **无**  | 分层Softmax     | 中心词 -> 上下文 | **低** (比CBOW略高) | `C × (D + D × log₂(V))`           |

**核心演进路径**:

1.  **优化输出层**: 用 **分层Softmax** 替换标准 Softmax。
2.  **优化隐藏层**: 直接**移除隐藏层**，对于学习词向量这个目标而言，复杂的非线性组合可能是“杀鸡用牛刀”。
3.  **改变任务**: 放弃构建精确语言模型，设计出更简单、更专注的“伪任务”，其唯一目的就是训练出高质量的词向量。
4.  **思想：当数据量足够大时，模型的简单性带来的训练效率优势，往往能胜过复杂模型带来的精度优势。一个在万亿级语料上训练的简单模型 (CBOW)，其产出的词向量质量，远超一个只能在十亿级语料上训练的复杂模型 (NNLM)**

## GloVe

### 链接与要点

链接（拓展）：[单词向量空间_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Fr4y1w7mE?spm_id_from=333.788.videopod.episodes&vd_source=3e32a51d0489203920eb4337d8de4c31&p=2)

要点：在理解bag-of-word的基础上，先知道模型是如何设计的，再深究 怎么得到模型的、为什么这样有道理。

PS：原论文是完整的思路，从启发点开始一步步推导；我的话让gemini一句一句翻译论文并解析看的，没看懂也硬着头皮继续看，然后柳暗花明。

### GloVe模型组件

GloVe模型的核心是一个**加权最小二乘回归模型**。它的目标是最小化一个**代价函数 (Cost Function)** `J`。（先记下来：权重f()，词向量w，偏置b，共现次数X_ij）

$J = Σ f(X_{ij}) * (w_iᵀ * ~w_j + b_i + ~b_j - log(X_{ij}))²$

1. **输入数据 - 共现矩阵 `X`**:

   - 这是模型训练前需要**提前准备好**的唯一数据。
   - `X_ij` 代表在整个语料库中，单词 `j` 出现在单词 `i` 上下文窗口中的次数。
   - **举例**：
     - 语料库(Corpus):"I like deep learning."，"I like NLP."，"I enjoy learning."
       
     - **词汇表**: { I, like, deep, learning, NLP, enjoy }

     - **上下文窗口大小**: 1
     - 共现矩阵如下：

   | `X`          |  I   | like | deep | learning | NLP  | enjoy |
   | :----------- | :--: | :--: | :--: | :------: | :--: | :---: |
   | **I**        |  0   |  2   |  0   |    0     |  0   |   1   |
   | **like**     |  2   |  0   |  1   |    0     |  1   |   0   |
   | **deep**     |  0   |  1   |  0   |    1     |  0   |   0   |
   | **learning** |  0   |  0   |  1   |    0     |  0   |   1   |
   | **NLP**      |  0   |  1   |  0   |    0     |  0   |   0   |
   | **enjoy**    |  1   |  0   |  0   |    1     |  0   |   0   |

2. **待学习的参数**:

   - $w_i$: 目标词 `i` 的**中心词向量**。
   - $~w_j$: 上下文词 `j` 的**上下文词向量**。
   - $b_i$ 和 $~b_j$: 分别是中心词和上下文词的**偏置项**，用于捕捉词本身的一些独立于上下文的特性（比如词频）。
   - **最终我们需要的词向量就是 `w` 或者 `w` 和 `~w` 的和/平均值。**

3. **核心方程**:

   - $w_iᵀ * ~w_j + b_i + ~b_j$：这是模型的**预测部分**。它用两个向量的点积和两个偏置项，来预测词 `i` 和 `j` 之间的关联度。
   - $log(X_{ij})$：这是模型的**目标部分**。它代表了我们希望预测值逼近的“真实”关联度，直接来源于全局的共现统计。

4. **权重函数 $f(X_{ij})$**:

   - 它是一个权重函数，根据共现次数`X_ij`的大小来决定某个词对在训练中的重要性，它会影响 损失的大小 从而影响 一次反向传播 对参数的 调整程度：

     - **`f(0) = 0`**: 对于从未共现的词对（$X_{ij}=0$），权重为0，完全不参与训练，极大降低计算复杂度。
     - **非递减**: 共现次数越多的词对，权重越大（但不是无限大），因为它们包含更可靠的信息。
     - **设置上限**: 对于共现次数极高的词对（如 "the", "is"），权重被限制在一个上限（比如1），防止这些高频停用词主导整个训练过程。
   - 文章给出分段函数如下图，$x_{max}$ 取100，`α` 取 3/4，满足上面的三个条件 f(0)=0, 非递减, 设置上限。
   
   <img src=".\glove_weight_func.png" alt="glove_weight_func" style="zoom:38%;" />


### GloVe训练过程

**第1步：构建共现矩阵**

1. **确定词汇表**：扫描整个语料库，统计词频，筛选出最高频的N个单词作为词汇表。

2. **确定上下文窗口**：设定一个窗口大小，比如左右各5个词。

3. **统计共现次数**: 遍历整个语料库，对于词汇表中的每一个词 `i`，查看其上下文窗口中出现了哪些词 `j`，并累加到 `X_ij` 上。

**第2步：权重和初始化参数**

1. 为词汇表中的每个词，随机初始化它的中心词向量 `w`、上下文词向量 `~w`、以及偏置项 `b` 和 `~b`。这些通常都是一些很小的随机数。
2. 根据权重函数和共现矩阵得到`f(X_ij)`。

**第3步：迭代优化**

1. **核心目标**：通过调整 `w`, `~w`, `b`, `~b` 这些参数，来让代价函数 $J = Σ f(X_{ij}) * (w_iᵀ * ~w_j + b_i + ~b_j - log(X_{ij}))²$ 的值变得尽可能小，**人话就是**：**加权后的 【行词向量和列词向量的内积 + 偏置】 接近 【相应行列共现矩阵中的值】，达到整体误差最小的地步**。
2. **优化算法**：通常使用随机梯度下降 (SGD) 的变种，如 **AdaGrad** 算法。
3. 具体流程:
   - 从共现矩阵 `X` 中随机抽取一个非零的词对 `(i, j)` 及其共现次数 $X_{ij}$。
   - 计算这对词的预测误差: $error = (w_iᵀ * ~w_j + b_i + ~b_j) - log(X_{ij})$。
   - 根据 $X_{ij}$ 计算权重: $weight = f(X_{ij})$。
   - 计算这个样本的**加权损失**: `loss = weight * error²`。
   - 根据这个损失，计算它对涉及到的参数 ($w_i, ~w_j, b_i, ~b_j$) 的**梯度**（即每个参数应该调整的方向）。
   - 沿着梯度的反方向，**微小地更新**这些参数，使得下次计算这对词时，误差会变小一点。
4. **重复**: 不断重复第3步，遍历所有非零的词对，进行多轮（epochs）训练，直到代价函数 `J` 的值收敛（不再显著下降）。

**第4步：获得最终词向量**

1. 训练结束后，我们就得到两套词向量矩阵 `W` (所有 `w` 的集合) 和 `~W` (所有 `~w` 的集合)。
2. 最终的词向量通常是这两套向量的**和**或**平均值**，即 $W_{final} = W + ~W$。这样做可以融合两方面的信息，使得到的词向量更加鲁棒。

### 哲学思想

- 原论文提到的共现率比例体现哪里？$J = Σ f(X_{ij}) * (w_iᵀ * ~w_j + b_i + ~b_j - log(X_{ij}))²$这个式子只涉及i j，压根没有k！        

  答曰：要区分“**模型的思想起点**”和“**最终的数学形式**”。比率是 GloVe 模型的**设计哲学和推导工具**，而不是它在训练时直接优化的目标。（ps：具体的问大模型去吧）

- 那glove和LSA有什么关系？没有是什么矩阵分解啊？      

  答曰：`J = Σ f(X_ij) * (w_iᵀ * ~w_j + b_i + ~b_j - log(X_ij))²` 这个公式看起来不是常见的矩阵分解形式，如 `X ≈ W * Hᵀ`，**但它本质上是一种“加权的矩阵分解”**。

  1. **简化问题**：

     - 暂时忽略权重 $f(X_{ij})$ 和偏置项 $b_i$, $~b_j$。
   - 那么损失函数就变成了 $J = Σ (w_iᵀ * ~w_j - log(X_{ij}))²$。
  
  2. **理解$w_iᵀ * ~w_j$的矩阵形式**：

     - $w_i$ 是词向量矩阵 $W$ 的第 `i` 行。
   - $~w_j$ 是词向量矩阵 $~W$ 的第 `j` 行。
     - 那么 $w_iᵀ * ~w_j$ 就是矩阵乘积 $W * ~Wᵀ$ 之后得到的那个大矩阵中，处在第 `i` 行、第 `j` 列的那个元素。
  
  3. **重新表述损失函数**：

     - 我们上面简化的损失函数 $J = Σ (w_iᵀ * ~w_j - log(X_{ij}))²$，其实就是在说：

       > “找到两个低维矩阵 `W` 和 `~W`，使得它们的乘积 `W * ~Wᵀ` 尽可能地接近目标矩阵 `log(X)`。”

     - 用矩阵的范数来表示，这就是在最小化 `|| W * ~Wᵀ - log(X) ||_F²` (弗罗贝尼乌斯范数的平方)。

- PS：6666666666666666666

## Seq2seq

### 前置知识、链接与要点

前置知识 由高到低：deep LSTMs -> LSTM -> 传统RNN -> MLP，MLP是零基础可以直接学的。

**必看链接**：[RNN大白话讲解](https://www.bilibili.com/video/BV1e5411K7oW/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)、[LSTM大白话讲解 ](https://www.bilibili.com/video/BV1qM4y1M7Nv/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)、[序列到序列学习【动手学深度学习v2】](https://www.bilibili.com/video/BV16g411L7FG/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

要点：

- 理解RNN里的“时间步上的展开”
- 理解LSTM通过**细胞状态C**进行“遗忘和更新”
- 理解**encoder-decoder**架构
- 其他注意点：多层LSTM，encoder-decoder架构在训练和实际预测时的不同、让大模型进行矩阵计算过程模拟，反向传播涉及矩阵微积分（我投降，我用维度分析法），梯度爆炸（我再投降，你能拿我咋滴）



**下文遵循 "MLP(究极粗糙版) -> MLP(滑窗版) -> 传统RNN  -> LSTM -> deep LSTMs ->encoder-decoder" 的顺序**



### MLP

场景设定为：对于一个序列/向量 比如 a cat sat on a ___ 这里要预测个词

对于**MLP(究极粗糙版)**，需要将a cat sat on a转成5个词嵌入 5*(1, 300)，然后拼接形成(1, 1500)，然后经过隐藏层 (1, 1500) × (1500, 128) = (1, 128)，最后输出层 (1, 128) × (128, 10000) ，这里假定10000个词，然后softmax看哪个词。

这里问题在于， 这里定长了，于是有**MLP(滑窗版)**，假设窗口长度为3。若训练数据为“a cat sat on a mat”  则滑窗出 a cat sat (on)、cat sat on (a)、sat on a (mat) 来训练。预测时，只能利用最后三个词“sat on a”去预测 mat。

公式：$â = tanh(W_{ax} × x+b_{a})$	$ŷ = softmax(W_{ya} × â + b_y)$

### 传统RNN

先看 [RNN大白话讲解](https://www.bilibili.com/video/BV1e5411K7oW/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

场景依旧 “a cat sat on a ___”，传统RNN初始参数有：词嵌入表（冻结）（1e4, 300），隐藏层权重（428, 128），隐藏层偏置 (1, 128)，输出层权重（128, 1e4），输出层偏置 (1, 1e4)，其中 1e4 为“假设词汇表共个1e4个词”，128 是隐藏层神经元个数，300是词向量维度。

先将 a 转成词嵌入 (1, 300)，与上个隐藏层结果（h_0设定为全0 (1, 128)）拼接得到 (1, 428)，经过隐藏层 (1, 428) × (428, 128) = (1, 128)  = h1；cat转为词嵌入 (1, 300)，与上个隐藏层结果得到 (1, 428)，经过隐藏层 (1, 428) × (428, 128) = (1, 128)  = h2 ；sat、on、a 类似；经过 a 后有 h5 = (1, 128)，经过输出层 (128, 1e4)，然后softmax后知道是哪个词。

训练时，h1~h5都会经过输出层得出L1~L5（语言建模） 或者 只在最后一个时间步产生一个输出和损失（情感分类），前者 总损失 通过**求和** $L = L_1 + L_2 + L_3$ (或取平均)，后者 总梯度(如对矩阵W_hh)通过**传递**: $∂L/∂W_{hh} = (∂L/∂W_{hh})_{t=1} + (∂L/∂W_{hh})_{t=2} + (∂L/∂W_{hh})_{t=3}$

公式：	$h_{i} = tanh(W_{hx} × (x_i+h_{i-1})+b_{h})$   

​		  $ŷ_i = softmax(W_{yh} × h_i + b_{y})$

### LSTM

一句话就是，LSTM平时多传个变量“细胞状态$C_t$”然后多弄几个参数矩阵相应地处理它。

理解传统RNN后看 [LSTM大白话讲解  ](https://www.bilibili.com/video/BV1qM4y1M7Nv/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930) 就能理解，不理解就再看RNN

公式记忆 ：一个 隐状态 ，一个 细胞状态，细胞遗忘 `× σ`，细胞更新 `+ (σ × tanh)`，新隐状态生成 $h_t = tanh(C_t) × σ$ 

视频里的公式讲解图如下

<img src=".\lstm_equations.png" alt="lstm_equations" style="zoom:38%;" />

### deep LSTMs

```
                               o_t ---> ...   (最终输出)
                                ^
                                |
      h3_t-1 ---> h3_t ---> h3_t+1  (第3层 - 最高层抽象)
         ^         ^          ^
         |         |          |
      h2_t-1 ---> h2_t ---> h2_t+1  (第2层 - 中间层抽象)
         ^         ^          ^
         |         |          |
      h1_t-1 ---> h1_t ---> h1_t+1  (第1层 - 底层特征)
         ^         ^          ^
         |         |          |
        x_t-1     x_t        x_t+1      (原始输入序列)
```

假设维度大小 —— 词向量(1,300) 隐藏状态(1,128)，则第一层的某个权重矩阵大小为(300+128, 128)，其他层(128+128, 128)。

### encoder-decoder(Seq2seq)

先看 [序列到序列学习【动手学深度学习v2】](https://www.bilibili.com/video/BV16g411L7FG/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

<img src=".\seq2seq.png" alt="seq2seq" style="zoom:40%;" />

- 蓝色部分为encoder即一个LSTM，白色部分为decoder也是一个LSTM。
- encoder的隐状态作为decoder的$h_0$，（预测时）给decoder一个“开始记号”为$<bos>$，第一个时间步里输出的词作为第二个时间步输入词，如果输出的词为$<eos>$，则结束
- 图中hello world .如果反着输入会有更好效果 —— “这点上我觉得是，解码器它（除t=1）后面的RNN层输入的词是 t-1 输出的词，而hello 最后进入，t=1 当然更容易是hello 相关，这样t=2也更正确，后续亦如此”
- Seq2seq论文中使用4层深度的 deep LSTMs，也就是$h4_t$作为输出
- 训练时，在decoder这里会保证每个时间步输入的都是对的词，以调整参数

## Bahdanau Attention

Bahdanau /bɑːdˈɑːnaʊ/ ，“吧嗒脑”注意力，  加法注意力

### 链接与要点

**链接：**[使用注意力机制的seq2seq](https://www.bilibili.com/video/BV1v44y1C7Tg/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

**要点：**双向RNN，论文中的注解、$g(y_{i-1}, s_i, c)$ 的 c 的固定传入、$g(y_{i-1}, s_i, c_i)$ 的 $c_i$ 的产生与作用、**加性注意力机制 **$W\tanh(W_q×s + W_k × h)$ (对应于论文中$e_{ij}$的计算)

提示词积累：   
```
把相关公式全部一次性列出”、
这些公式一行用原公式，一行用具体矩阵（m, n）× 这样的方式呈现，注意公式的成功显示
```

### 核心公式概览

<img src="/birnn.png" alt="image-20251126195838527" style="zoom:33%;" />

假设我们在时间步 t 解码目标词 $y_t$，此时传入前一步的隐藏状态$S_{t-1}$，编码器的所有注解是 $(h_1, h_2, ..., h_{T})$。（注解的概念需结合双向RNN，一句话就是“biRNN产生的“同词隐状态”拼接”）

1.  **计算能量分数 (Energy Score):**
    $e_{ij} = a(s_{i-1}, h_j)$
    说白了一个向量反映 $s_{i-1}$ 与 $(h_1, h_2, ..., h_{T})$ 的匹配程度。

2.  **计算注意力权重 (Attention Weights):**
    $\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}$
    说白了 Softmax。

3.  **计算上下文向量 (Context Vector):**
    $c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j$
    说白了 $(h_1, h_2, ..., h_{T})$ 叉乘 注意力向量。

4.  **计算解码器新状态 (Decoder Hidden State):**
    $s_i = f(s_{i-1}, y_{i-1}, c_i)$
    生成当前时间步隐状态。

5.  **预测目标词概率 (Output Prediction):**
    $p(y_i | y_{<i}, \mathbf{x}) = g(y_{i-1}, s_i, c_i)$

### 设定具体维度

为了让矩阵的尺寸更加清晰，我们首先设定一组具体的、符合常规的维度值：

*   **RNN隐状态维度 ($d_{rnn}$):** `128`
*   **编码器:** 双向RNN (BiRNN)，隐状态维度: 128
*   **编码器注解维度 ($d_h$):** `256`
    *   因为是双向RNN，每个时间步的注解 $h_j$ 是由前向状态 $\overrightarrow{h_j}$ 和后向状态 $\overleftarrow{h_j}$ 拼接而成，所以 $d_h = 128 + 128 = 256$。
*   **解码器隐状态维度 ($d_s$):** `128`
    *   论文中设定解码器RNN的隐状态维度与编码器单向RNN的隐状态维度相同。
*   **注意力机制隐维度 ($d_{att}$):** `64` (这是一个超参数，可以自己设定)
*   **词嵌入维度 ($d_{embed}$):** `100`
*   **源句长度 ($T_x$):** `10` (比如 "I am a student from China")
*   **目标词汇表大小 ($V_{target}$):** `10000`

### 分步公式与维度详解

**1. 计算 e**

原始公式(**加性注意力**)：

$$
e_{ij} = v_a^\top \tanh(W_a s_{i-1} + U_a h_j)
$$

向量化形式：
$$
\underset{(1 \times 10)}{\mathbf{e}_i} = \underset{(1 \times 64)}{v_a^\top} \tanh \left( \underset{(64 \times 128)}{W_a} \underset{(128 \times 1)}{s_{i-1}} + \underset{(64 \times 256)}{U_a} \underset{(256 \times 10)}{\mathbf{H}} \right)
$$

*   $s_{i-1}$ (解码器状态) 是一个 `(128, 1)` 的列向量。
*   $\mathbf{H}$ (编码器所有注解) 是一个 `(256, 10)` 的矩阵，每一列是一个 $h_j$。
*   $W_a s_{i-1}$ 计算结果为 `(64, 1)`。
*   $U_a \mathbf{H}$ 计算结果为 `(64, 10)`。
*   加法时，`(64, 1)` 的向量被**广播**成 `(64, 10)` 的矩阵，与 `(64, 10)` 的矩阵相加。
*   最终，$v_a^\top$ `(1, 64)` 乘以 `tanh` 后的 `(64, 10)` 矩阵，得到 `(1, 10)` 的能量分数行向量 $\mathbf{e}_i$。

**2. 计算注意力权重 α**

**维度详解:**
$$
\underset{(1 \times 10)}{\boldsymbol{\alpha}_i} = \text{Softmax} \left( \underset{(1 \times 10)}{\mathbf{e}_i} \right)
$$

*   Softmax函数作用于行向量 $\mathbf{e}_i$ `(1, 10)` 的每一个元素上。
*   输出的 $\boldsymbol{\alpha}_i$ 仍然是一个 `(1, 10)` 的行向量，其中每个元素 $\alpha_{ij}$ 都在0到1之间，且所有元素之和为1。

**3. 计算上下文向量 c**

**维度详解:**
$$
\underset{(256 \times 1)}{c_i} = \underset{(256 \times 10)}{\mathbf{H}} \cdot \underset{(10 \times 1)}{\boldsymbol{\alpha}_i^\top}
$$

*   我们将注意力权重行向量 $\boldsymbol{\alpha}_i$ `(1, 10)` 转置为列向量 $\boldsymbol{\alpha}_i^\top$ `(10, 1)`。
*   然后用编码器注解矩阵 $\mathbf{H}$ `(256, 10)` 乘以这个权重列向量。
*   这个矩阵乘法完美地实现了加权求和的功能，得到了一个维度为 `(256, 1)` 的上下文向量 $c_i$。

**4. 计算隐状态 s**

**通用公式:**
$$
s_i = f(s_{i-1}, y_{i-1}, c_i)
$$

**维度详解:**
$$
\underset{(128 \times 1)}{s_i} = \text{RNN} \left( \underset{(128 \times 1)}{s_{i-1}}, \underset{(356 \times 1)}{\text{input}_i} \right) \quad \text{其中 } \text{input}_i = \text{concat}(E y_{i-1}, c_i)
$$

**注解:**

*   $y_{i-1}$ 是上一步预测的词的ID，通过一个嵌入层 $E$ 变为词嵌入向量 $E y_{i-1}$，维度为 `(100, 1)`。
*   将词嵌入向量 $E_{y_{i-1}}$`(100, 1)` 与上下文向量 $c_i$ `(256, 1)` **拼接 (concatenate)** 起来，形成RNN的实际输入 `input_i`，维度为 `(100 + 256, 1) = (356, 1)`。
*   解码器的RNN单元接收上一个隐状态 $s_{i-1}$ `(128, 1)` 和这个新的输入 `input_i` `(356, 1)`，经过内部运算后，输出新的隐状态 $s_i$，维度依然是 `(128, 1)`。

**5. 预测目标词概率 P(y)**

**通用公式:**
$$
p(y_i | y_{<i}, \mathbf{x}) = g(y_{i-1}, s_i, c_i)
$$
(原论文中 `g` 是一个带有Maxout的深度输出层，这里简化为常见的线性层+Softmax)

**维度详解:**
$$
\underset{(10000 \times 1)}{\mathbf{p}_i} = \text{Softmax} \left( \underset{(10000 \times 484)}{W_{out}} \cdot \underset{(484 \times 1)}{\text{concat}(E y_{i-1}, s_i, c_i)} + \underset{(10000 \times 1)}{b_{out}} \right)
$$

**注解:**
*   将上一步的词嵌入 $E y_{i-1}$ `(100, 1)`、当前解码器状态 $s_i$ `(128, 1)` 和上下文向量 $c_i$ `(256, 1)` 三者拼接。
*   拼接后的向量维度为 `(100 + 128 + 256, 1) = (484, 1)`。
*   这个向量通过一个大的输出线性层（权重矩阵 $W_{out}$ 尺寸为 `(10000, 484)`，偏置 $b_{out}$ 尺寸为 `(10000, 1)`）进行变换。
*   变换结果是一个 `(10000, 1)` 的向量，称为 logits。
*   最后，对 logits 应用Softmax函数，得到词汇表中每个词的概率分布向量 $\mathbf{p}_i$，维度为 `(10000, 1)`。概率最高的那个词就是我们这一步的最终预测结果。

### 反向传播

**可视化梯度流：**
$$
L \xrightarrow{\text{梯度}} p(y_1) \xrightarrow{\text{梯度}} s_1 \xrightarrow{\text{梯度}} c_1 \xrightarrow{\text{梯度}} \alpha_1 \xrightarrow{\text{梯度}} e_1 \xrightarrow{\text{梯度}} \text{对齐模型 a 的参数}
$$

## Seft-attention

### 链接、要点与思考

**链接**：[深入浅出：用中学数学理解Transformer模型（必看）](https://www.bilibili.com/video/BV15w4m1R7E8/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)、[李宏毅 | 自注意力机制和Transformer详细解析（必看）](https://www.bilibili.com/video/BV1r8nMz4EAj/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)、[李沐自注意力](https://www.bilibili.com/video/BV19o4y1m7mo/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

**要点**：自注意力机制思想、点积注意力公式、transformer架构、因果mask实现并行

思考与拓展：理解完全部组件再回过头，encoder就是将**原始输入序列进行一个上下文关联化**，它的输出类似RNN原序列里的各个隐藏层状态(h_1...h_n)，训练时decoder则通过因果mask，使得**每个正确答案token都受到上文关联化**，然后每个token的loss能并行获取到，预测时则串行地从仅输入\<s\>开始，每次都从最后一个token去获取下一个词，直到遇到\</s\>。**拓展**源自李宏毅视频，比如 **guided attention** 比如 **beam tree** 比如 **训练和预测时加入噪音** 比如 **用blue score计算损失时无法求偏导用RL硬刚** 比如 scheduled sampling类似训练时加入噪音（模拟预测时预测到错误）增强鲁棒性

### 引入

没有上下文信息的labeling，这样两个saw必然输出相同词性

<img src=".\normal_fc_labeling.png" alt="normal_fc_labeling" style="zoom:25%;" />

给个窗口，拼接窗口内向量作为输入，提供上下文信息

没有考虑全文，无法变长，参数量大

<img src="D:\办公程序\github_blog\hugo_extended_withdeploy_0.141.0_windows-amd64\rublog\content\post\语言模型发展论文集\window_fc_labeling.png" alt="window_fc_labeling" style="zoom:25%;" />

自注意机制后的向量有上下文信息，由参数矩阵分配，可变长且全局。

<img src=".\self_atten_labeling.png" alt="self_atten_labeling" style="zoom:25%;" />

可多层“self-attention + FC”

<img src=".\more_self_attent_labeling.png" alt="more_self_attent_labeling" style="zoom:30%;" />

### 自注意机制模块

衡量两个词之间的“相关度”——点积（外积）

绿色方框词向量分别为 $I_1$ $I_2$，此处可以得出词向量的“相关度”，具体来说： **$I_1$  的每个值 和 $I_2$ 的 相关度向量 concat 形成 矩阵（外积）** ，公式如下，$d_k$ 是 k 的维度（ $I_1$  $I_2$ q k 维度一样大），过除以 $\sqrt{d_k}$，可以将**点积的方差**稳定在 1 左右，使其大小不受向量维度 $d_k$ 的影响。     

​                                                    $$ \alpha = \frac{(I_1 W^q)(I_2 W^k)^T}{\sqrt{d_k}}$$

<img src=".\dot_product.png" alt="dot_product" style="zoom:25%;" />

假设输入序列为 $A = [a^1, a^2, ..., a^n]$，输出序列为 $B = [b^1, b^2, ..., b^n]$。

1.  **生成Query, Key, Value**: 对于每个输入向量 $a^i$：
    $$ q^i = W^q a^i $$                  $$Q = A W^q$$
    $$ k^i = W^k a^i$$                 $$K = A W^k$$
    $$ v^i = W^v a^i$$                 $$V = A W^v$$

2. **计算注意力权重**: 计算第 $j$ 个输出 $b^j$ 时，需要用它的查询 $q^j$ 和所有的键 $k^i$ 计算相关性得分，然后通过Softmax进行归一化。

   **$A'$用我的话说：A的长度个 “描述相关性”的 向量，第 i 个向量描述 $b^i$ 如何由 $V$ 加权得来**

   $$ \alpha'_{j,i} = \text{softmax}(\frac{(q^j)^T k^i}{\sqrt{d_k}})$$                  $$A' = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})$$

3. **计算输出向量**: 将权重 $\alpha'_{j,i}$ 与对应的 $v^i$ 加权求和，得到输出 $b^j$。
   $$ b^j = \sum_{i=1}^{n} \alpha'_{j,i} v^i $$                       $$ B = A'V $$

<img src=".\self_attent_a2b.png" alt="self_attent_a2b" style="zoom:25%;" />

<img src=".\self_atten.png" alt="self_atten" style="zoom:25%;" />

### 位置编码

这里是一个位置编码例子（来自）发现2j和2j+1的区别是sin和cos，也就是说：对第 i 个向量进行编码 第0，1维变化是 "+b" (sin(wx+b)里)，即平移；第0，2维变化是 "w+1"，即伸缩。

维度稍微大点，如300，就几乎能确保 同向量里 位置编码 不重复（把这个几个sin cos 用傅里叶变换 可以变成一个新的 sin，所以是有周期性的）。 

这个编码的好处是，有个“相对性”变换，直接不停地对前一个位置乘矩阵就行。
$$
\begin{bmatrix}
\cos(\delta\omega_j) & \sin(\delta\omega_j) \\
-\sin(\delta\omega_j) & \cos(\delta\omega_j)
\end{bmatrix}
\begin{bmatrix}
p_{i,2j} \\
p_{i,2j+1}
\end{bmatrix}
=
\begin{bmatrix}
p_{i+\delta,2j} \\
p_{i+\delta,2j+1}
\end{bmatrix}
$$
<img src=".\position_coding.png" alt="position_coding" style="zoom:25%;" />

### transformer-encoder

拆解并量化 Transformer 的每一个步骤，将设定一些具体的维度值，这些值与原始论文《Attention Is All You Need》中的设定类似。

#### 设定参数

*   输入序列长度 (Sequence Length): $T = 4$。例如，输入一句话 "I am a robot"。

*   词嵌入维度 (Embedding Dimension): $d_{\text{model}} = 512$。这是模型中大部分向量的维度。

*   注意力头数 (Number of Heads): $h = 8$。

*   每个头的维度 (Dimension per Head): $d_k = d_v = d_{\text{model}} / h = 512 / 8 = 64$。

*   前馈网络中间层维度 (Feed Forward Inner Dimension): $d_{\text{ff}} = 2048$。
*   （关于头的维度：让他们关注不同的“子空间”（subspace），我们不给每个“头”完整的512维信息，而是把这个信息“切分”成8份。每一份是64维（`512 / 8 = 64`）。这样，每个“头”就只能在自己被分配到的64维信息空间里去学习和计算注意力。）

#### 嵌入层和位置编码 

Embedding & Positional Encoding，假设我们有一个长度为 4 的输入序列。

1.  **Input Embedding**: 首先，我们将每个词（token）通过一个嵌入层（本质上是一个查找表）转换成一个向量。
    *   **输入**: 4 个词的 ID。
    *   **输出**: 一个矩阵 $X_{\text{emb}}$， $X_{\text{emb}} \in \mathbb{R}^{4 \times 512}$ (即 $T \times d_{\text{model}}$)，代表了每个词的语义嵌入。

2.  **Positional Encoding**: 我们生成一个与 $X_{\text{emb}}$ 相同维度的位置编码矩阵 $P$，然后和词嵌入逐元素相加，得到进入 Encoder Block 的最终输入矩阵 $X$。
    *   **维度**: $P \in \mathbb{R}^{4 \times 512}$。
        *   $$ X_{\text{emb}} + P = X \in \mathbb{R}^{4 \times 512}$$


#### 1. MHA

   标准的 Multi-Head Attention (MHA)是 Encoder 的第一个核心子层。它的输入是矩阵 $X$。    
$$
X \xrightarrow{} Q,K,V \xrightarrow{\text{split}}  Q',K',V'  \xrightarrow{·} \text{Head}_{\text{out}} \xrightarrow{\text{concat}} Z \xrightarrow{\text{$W^O$}} \text{MHA}_{\text{out}}
$$

**3.1 生成 Q, K, V 矩阵**

通过将输入 $X$ 与三个不同的权重矩阵 $W^Q, W^K, W^V$ 相乘来实现，也就是完整的K Q V。

*   **输入**: $X \in \mathbb{R}^{4 \times 512}$
*   **权重矩阵**:
    *   $W^Q \in \mathbb{R}^{512 \times 512}$
    *   $W^K \in \mathbb{R}^{512 \times 512}$
    *   $W^V \in \mathbb{R}^{512 \times 512}$
    *(注意: $W^Q$, $W^K$, $W^V$ 的维度都是 $d_{\text{model}} \times d_{\text{model}}$)*

*   **矩阵运算**:
    $$ Q = X W^Q \quad (4, 512) \times (512, 512) \rightarrow (4, 512) $$
    $$ K = X W^K \quad (4, 512) \times (512, 512) \rightarrow (4, 512) $$
    $$ V = X W^V \quad (4, 512) \times (512, 512) \rightarrow (4, 512) $$
*   **输出**: $Q, K, V \in \mathbb{R}^{4 \times 512}$。

**3.2 拆分成多个头**

接下来，我们将 $Q, K, V$ 矩阵在最后一个维度（512）上拆分成 8 个头，每个头的维度是 64。

*   **操作**: Reshape 和 Transpose。将 $(4, 512)$ 变为 $(4, 8, 64)$，然后交换头和序列长度的维度，方便后续计算。     
    
    $Q \rightarrow Q_{\text{split}} \in \mathbb{R}^{8 \times 4 \times 64}$ (即 $h \times T \times d_k$)     
    
    $K \rightarrow K_{\text{split}} \in \mathbb{R}^{8 \times 4 \times 64}$      
    
    $V \rightarrow V_{\text{split}} \in \mathbb{R}^{8 \times 4 \times 64}$      

**3.3 计算缩放点积注意力 (Scaled Dot-Product Attention)**

这一步 **每个头独立进行** 的，以其中一个头为例，其输入 $Q', K', V'$ 的维度都是 $(4, 64)$。

1.  计算得分 (Scores):
    $$ \text{Scores} = Q' (K')^T \quad (4, 64) \times (64, 4) \rightarrow (4, 4) $$
    这个 $(4,4)$ 的矩阵表示序列中每个词对其他所有词（包括自己）的注意力原始得分。

2.  缩放 (Scale):
    $$ \text{Scores}_{\text{scaled}} = \frac{\text{Scores}}{\sqrt{d_k}} = \frac{\text{Scores}}{\sqrt{64}} = \frac{\text{Scores}}{8} $$
    逐元素除法，仍为 $(4,4)$。

3.  Softmax: 对得分矩阵的每一行应用 Softmax，得到注意力权重矩阵 $A'$。
    $$ A' = \text{softmax}(\text{Scores}_{\text{scaled}}) \in \mathbb{R}^{4 \times 4} $$
    $A'$ 的每一行元素相加为 1。

4.  加权求和: 将权重矩阵 $A'$ 与 $V'$ 相乘。
    $$ \text{Head}_{\text{out}} = A' V' \quad (4, 4) \times (4, 64) \rightarrow (4, 64) $$
    $\text{Head}_{\text{out}}$ 就是这个头计算出的结果。

**3.4 合并多头结果**

Concat 和 Reshape。

*   将 8 个 $(4, 64)$ 的矩阵在最后一个维度上拼接，得到一个拼接后的 $(4, 512)$ 的矩阵$Z$
*   $$(8, 4, 64) \rightarrow (4, 512) = Z \in \mathbb{R}^{4 \times 512}$$

**3.5 最终线性变换**

将拼接后的矩阵 $Z$ 通过最后一个权重矩阵 $W^O$ 进行线性变换，得到多头注意力的最终输出。

*   权重矩阵: $W^O \in \mathbb{R}^{512 \times 512}$
*   矩阵运算:$$ \text{MHA}_{\text{out}} = Z W^O \quad (4, 512) \times (512, 512) \rightarrow (4, 512)\quad \text{MHA}_{\text{out}} \in \mathbb{R}^{4 \times 512} $$

#### 2. Add & Norm  - 1

残差连接与层归一化，这一层连接在 Multi-Head Attention 之后。

**4.1 Add (残差连接)**

将 Multi-Head Attention 层的输入 $X$ 与其输出 $\text{MHA}_{\text{out}}$ 逐元素相加。

$$ \text{Add}_{\text{out}} = X + \text{MHA}_{\text{out}} $$
*   维度: $(4, 512) + (4, 512) \rightarrow (4, 512)$。
*   作用: 缓解信息丢失。

**4.2 Norm (Layer Normalization)**

在**特征维度**上进行归一化，相当于一个词向量内部进行归一化。

*   输入: $\text{Add}_{\text{out}} \in \mathbb{R}^{4 \times 512}$。
*   操作: LN 会独立地对矩阵中的 **每一行** (每个词向量) 进行操作。我们以第一行向量 $v_1 \in \mathbb{R}^{512}$ 为例：
    1.  Mean: 计算向量 $v_1$ 中所有 512 个元素的均值 $\mu_1$。
        $$ \mu_1 = \frac{1}{512} \sum_{k=1}^{512} v_{1,k} $$
    2.  Variance: 计算向量 $v_1$ 中所有 512 个元素的方差 $\sigma_1^2$。
        $$ \sigma_1^2 = \frac{1}{512} \sum_{k=1}^{512} (v_{1,k} - \mu_1)^2 $$
    3.  归一化 (Normalize): 对 $v_1$ 的每个元素进行归一化。$\epsilon$ 是一个很小的数（如 $10^{-5}$），防止分母为零。
        $$ \hat{v}_{1,k} = \frac{v_{1,k} - \mu_1}{\sqrt{\sigma_1^2 + \epsilon}} $$
    4.  缩放与平移 (Scale and Shift): 将归一化后的向量乘以可学习的缩放参数 $\gamma$ (gamma) 并加上平移参数 $\beta$ (beta)。这两个参数的维度也是 $512$，**它们在模型训练过程中学习**，让网络可以恢复或调整归一化后的特征表示。
        $$ \text{LN}(v_1)_k = \gamma_k \hat{v}_{1,k} + \beta_k $$
    
*   LN 对输入的 $(4, 512)$ 矩阵的 **每一行** 都执行上述步骤，最终输出一个维度同样为 $(4, 512)$ 的矩阵，我们称之为 $\text{LN}_1$。

#### 3. FFN

这是 Encoder 的第二个核心子层，它是一个简单的两层全连接网络，被称为 **Position-wise Feed-Forward Network**，你可以把它想象成对 $(4, 512)$ 矩阵的每一行向量（512维）都应用了同一个 `512 -> 2048 -> 512` 的双层 MLP。

*   **输入**: $\text{LN}_1 \in \mathbb{R}^{4 \times 512}$。
*   **结构**:
    1.  第一个线性层（扩展维度）
    2.  ReLU 激活函数
    3.  第二个线性层（缩减维度）
*   **权重矩阵**:
    *   $W_1 \in \mathbb{R}^{512 \times 2048}$ (即 $d_{\text{model}} \times d_{\text{ff}}$)
    *   $b_1 \in \mathbb{R}^{2048}$ (偏置)
    *   $W_2 \in \mathbb{R}^{2048 \times 512}$ (即 $d_{\text{ff}} \times d_{\text{model}}$)
    *   $b_2 \in \mathbb{R}^{512}$ (偏置)
*   **矩阵运算**:
    $$ \text{FFN}_{\text{out}} = \text{ReLU}(\text{LN}_1  W_1 + b_1) W_2 + b_2 $$
    我们分解来看：
    1.  $\text{Layer}_1 = \text{LN}_1 W_1 + b_1$    $(4, 512) \times (512, 2048) \rightarrow (4, 2048)$。
    2.  $\text{Layer}_1 = \text{ReLU}(\text{Layer}_1)$    仍为 $(4, 2048)$。
    3.  $\text{Layer}_2 = \text{Layer}_1 W_2 + b_2$    $(4, 2048) \times (2048, 512) \rightarrow (4, 512)$。
*   **输出**: $\text{FFN}_{\text{out}} \in \mathbb{R}^{4 \times 512}$。

#### 4. Add & Norm - 2

与 Part 1 完全相同，只是输入不同。

1.  **Add**: 将 Feed Forward 层的输入 $\text{LN}_1$ 与其输出 $\text{FFN}_{\text{out}}$ 相加。
    $$ \text{Add}_ {\text{out}} = \text{LN}_1 + \text{FFN}_{\text{out}} $$	 $(4, 512) + (4, 512) \rightarrow (4, 512)$
    
1.  **Norm**: 对 $\text{Add}_ {\text{out}}$ 的每一行进行层归一化。
    $$ \text{EncoderBlock}_{\text{out}} = \text{LayerNorm}(\text{Add}_ {\text{out}}) $$	 $(4, 512)$
    

这个 $\text{EncoderBlock}_{\text{out}}$ 就是一个 Encoder Block 的最终输出。它可以作为下一个 Encoder Block 的输入，重复上述所有步骤 N 次。

### transformer-decoder （预测）

#### 场景设定

一些继续使用之前 Encoder 的设定：

*   词嵌入维度: $d_{\text{model}} = 512$
*   注意力头数: $h = 8$
*   每个头的维度: $d_k = d_v = 64$
*   词汇表大小 (Vocabulary Size): $V_{\text{size}} = 30000$ (用于最后的 Linear+Softmax)

假设 Encoder 已经运行完毕，输出了一个矩阵 $E_{\text{out}} \in \mathbb{R}^{4 \times 512}$ (对应 "I am a robot" 的 4 个词)。

现在我们开始 Decoder 的预测过程，目标是生成 "我 是 一 个 机器人"，时间步 **t=3** (假设已经生成了 "我 是")

*   **Decoder 输入**: `<s>` "我" "是"，序列长度为 3。
*   **矩阵维度**: $Y \in \mathbb{R}^{3 \times 512}$ (经过 Embedding 和 Positional Encoding)。

#### 1. Masked MHA

* **输入**: $Y \in \mathbb{R}^{3 \times 512}$。

*   **Q, K, V 生成**:
    $$ Q_d = Y W^Q_d \quad (3, 512) \times (512, 512) \rightarrow (3, 512) $$
    $$ K_d = Y W^K_d \quad (3, 512) \times (512, 512) \rightarrow (3, 512) $$
    $$ V_d = Y W^V_d \quad (3, 512) \times (512, 512) \rightarrow (3, 512) $$
    
*   **多头注意力计算** (以单头为例，输入 $Q'_d, K'_d, V'_d \in \mathbb{R}^{3 \times 64}$):
    
    1. Scores: $Q'_d (K'_d)^T \rightarrow (3, 64) \times (64, 3) = (3, 3)$
    
    2. **Masking** & Softmax: 将得分矩阵的上三角部分（不含对角线）设为 $-\infty$。    
    
       *   注意力得分矩阵 ($3 \times 3$):
           $$ \text{Scores} = \begin{bmatrix} s_{11} & s_{12} & s_{13} \\ s_{21} & s_{22} & s_{23} \\ s_{31} & s_{32} & s_{33} \end{bmatrix} $$
       *   应用掩码后:
           $$ \text{Scores}_{\text{masked}} = \begin{bmatrix} s_{11} & -\infty & -\infty \\ s_{21} & s_{22} & -\infty \\ s_{31} & s_{32} & s_{33} \end{bmatrix} $$
       *   经过 Softmax 函数后:
           $$ A'_d=\text{Softmax}(\text{Scores}_{\text{masked}}) \rightarrow \begin{bmatrix} \alpha_{11} & 0 & 0 \\ \alpha_{21} & \alpha_{22} & 0 \\ \alpha_{31} & \alpha_{32} & \alpha_{33} \end{bmatrix} $$
    
       ​       $ A'_d \in \mathbb{R}^{3 \times 3} $
    
    3. 加权: $A'_d V'_d \rightarrow (3, 3) \times (3, 64) = (3, 64)$。
    
*   **合并与输出**: 经过拼接和最终线性变换，得到 $\text{MaskedMHA}_{\text{out}} \in \mathbb{R}^{3 \times 512}$。

#### 2. Add & Norm - 1

*   **Add**: $\text{Add}_1 = Y + \text{MaskedMHA}_{\text{out}} \in \mathbb{R}^{3 \times 512}$
*   **Norm**: $\text{LN}_1 = \text{LayerNorm}(\text{Add}_1) \in \mathbb{R}^{3 \times 512}$

#### 3. MHA

Encoder-Decoder Attention (cross attention)，这是关键的交互步骤，一句话就是：根据 掩码自注意后的输入序列信息 动态汲取encoder输出的全局信息。

* **Query (Q)**: **来自 Decoder** 上一层的输出 $\text{LN}_1$。

* **Key (K) 和 Value (V)**: **来自 Encoder** 的最终输出 $E_{\text{out}}$。

*   **矩阵运算**:
    $$ Q_{ed} = \text{LN}_1 W^Q_{ed} \quad (3, 512) \times (512, 512) \rightarrow (3, 512) $$
    $$ K_{ed} = E_{\text{out}} W^K_{ed} \quad (4, 512) \times (512, 512) \rightarrow (4, 512) $$
    $$ V_{ed} = E_{\text{out}} W^V_{ed} \quad (4, 512) \times (512, 512) \rightarrow (4, 512) $$
    
*   **多头注意力计算** (以单头为例):
    *   $Q'_{ed} \in \mathbb{R}^{3 \times 64}$    $K'_{ed} \in \mathbb{R}^{4 \times 64}$    $V'_{ed} \in \mathbb{R}^{4 \times 64}$
    1.  **Scores**: $Q'_{ed} (K'_{ed})^T \rightarrow (3, 64) \times (64, 4) = (3, 4)$
        *   这个 $(3,4)$ 的矩阵代表 Decoder 当前的 3 个词分别对 Encoder 的 4 个输入词的关注度。
    2.  **Softmax**: 得到权重矩阵 $A'_{ed} \in \mathbb{R}^{3 \times 4}$。
    3.  **加权**: $A'_{ed} V'_{ed} \rightarrow (3, 4) \times (4, 64) = (3, 64)$。
    
*   **合并与输出**: $\text{EncDecMHA}_{\text{out}} \in \mathbb{R}^{3 \times 512}$。

#### 4. Add & Norm - 2

*   **Add**: $\text{Add}_2 = \text{LN}_1 + \text{EncDecMHA}_{\text{out}} \in \mathbb{R}^{3 \times 512}$
*   **Norm**: $\text{LN}_2 = \text{LayerNorm}(\text{Add}_2) \in \mathbb{R}^{3 \times 512}$

#### 5. FFN

*   **输入**: $\text{LN}_2 \in \mathbb{R}^{3 \times 512}$
*   **运算**: 与 Encoder 中的 FFN 完全一样。
    $$ \text{FFN}_{\text{out}} = \text{ReLU}(\text{LN}_2 W_1 + b_1) W_2 + b_2 $$
    *   **维度变化**: $(3, 512) \rightarrow (3, 2048) \rightarrow (3, 512)$
*   **输出**: $\text{FFN}_{\text{out}} \in \mathbb{R}^{3 \times 512}$

#### 6. Add & Norm - 3

*   **Add**: $\text{Add}_3 = \text{LN}_2 + \text{FFN}_{\text{out}} \in \mathbb{R}^{3 \times 512}$
*   **Norm**: $\text{DecoderBlock}_{\text{out}} = \text{LayerNorm}(\text{Add}_3) \in \mathbb{R}^{3 \times 512}$

这个 $\text{DecoderBlock}_{\text{out}}$ 会作为下一个 Decoder Block 的输入，**重复上述 1 - 6 过程 N 次**。

#### 7. final layer

Final Linear & Softmax，在经过 N 个 Decoder Block 后，我们得到最终的输出矩阵 $D_{\text{final}} \in \mathbb{R}^{3 \times 512}$。因为我们只关心下一个词的预测，所以我们只取这个矩阵的**最后一行**向量 $d_{\text{last}} \in \mathbb{R}^{1 \times 512}$ (对应输入 "是" 的位置)。

1.  **Linear**: 将这个 512 维的向量通过一个线性层，映射到词汇表大小的维度。
    *   **权重**: $W_{\text{final}} \in \mathbb{R}^{512 \times 30000}$
    *   **运算**: $ \text{Logits} = d_{\text{last}} W_{\text{final}} \quad (1, 512) \times (512, 30000) \rightarrow (1, 30000) $
2.  **Softmax**: 将 Logits 转换成概率分布。
    $$ \text{Probabilities} = \text{softmax}(\text{Logits}) \in \mathbb{R}^{1 \times 30000} $$

模型会从这个概率分布中选择概率最高的词（比如 "一"）作为下一个词的输出，然后进入 **时间步 t=4** 的循环。

### transformer-decoder （训练）

**训练中所有预测是在一次前向传播和一次反向传播中并行完成的，而不是 6 次独立的传播。**

*   **训练时**: **并行**处理，一次前向+一次反向，完成整个序列的学习。
*   **预测时**: **串行**处理，一次生成一个词，循环多次。

#### 1. 准备数据

*   **Decoder 输入矩阵 (6x512)**: 将 `<s>`, `我`, `是`, `一`, `个`, `机器人` 这 6 个 token 的词向量堆叠成一个矩阵（一次性完成）。
    $$
    Y = \begin{bmatrix}
    \text{emb}(<s>) \\
    \text{emb}(我) \\
    \text{emb}(是) \\
    \text{emb}(一) \\
    \text{emb}(个) \\
    \text{emb}(机器人)
    \end{bmatrix}
    $$
*   **目标标签 (Labels)**: 对应每个输入位置，我们期望模型预测出的下一个词。
    *   输入 `<s>` -> 期望输出 `我`
    *   输入 `我` -> 期望输出 `是`
    *   ...
    *   输入 `机器人` -> 期望输出 `</s>` (结束符)

#### 2. 一次前向传播

1.  **输入**: 整个 `Y` 矩阵 (6x512) 被一次性送入 Decoder。
2.  **Masked Multi-Head Attention**: 在注意力计算中，我们使用一个 6x6 的 **因果掩码 (Causal Mask)**。
    $$
    \text{Mask} = \begin{bmatrix}
    1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 \\
    1 & 1 & 1 & 0 & 0 & 0 \\
    1 & 1 & 1 & 1 & 0 & 0 \\
    1 & 1 & 1 & 1 & 1 & 0 \\
    1 & 1 & 1 & 1 & 1 & 1
    \end{bmatrix}
    $$
    (这里的 0 代表 $-\infty$，1 代表原始分数)

3.  **并行计算**: 这个掩码是关键！它确保了：
    *   在计算**第 1 行** (`<s>`) 的输出时，它只能看到自己。
    *   在计算**第 2 行** (`我`) 的输出时，它只能看到 `<s>` 和 `我`。
    *   在计算**第 6 行** (`机器人`) 的输出时，它可以看到所有 6 个输入。

    所有的这些计算都是通过矩阵乘法 **同时发生** 的。GPU 极其擅长这种大规模的并行计算。

4.  **输出**: Decoder 输出一个最终的矩阵 **`Z` (6x512)**。这个矩阵的每一行，都包含了在遵守“因果”规则下，对下一个词的预测信息。
    
    *   `Z[0]` 是基于 `<s>` 的预测。
    *   `Z[1]` 是基于 `<s> 我` 的预测。
    *   ...
    *   `Z[5]` 是基于 `<s> 我 是 一 个 机器人` 的预测。

#### 3. 一次损失计算

1.  我们将输出矩阵 `Z` (6x512) 通过最后的线性层和 Softmax，得到一个概率分布矩阵 **`Probs` (6 x vocab_size)**。
2.  我们**一次性**比较这个 `Probs` 矩阵和我们的目标标签：
    *   比较 `Probs[0]` 和 标签 `我` 的概率。
    *   比较 `Probs[1]` 和 标签 `是` 的概率。
    *   ...
    *   比较 `Probs[5]` 和 标签 `</s>` 的概率。
3.  我们将这 6 个位置的损失（通常是交叉熵损失）相加或求平均，得到一个 **最终的、单一的损失值**。

#### 4. 一次反向传播

我们根据这个**单一的损失值**，进行**一次**反向传播，计算所有模型参数（$W^Q, W^K, W^V$ 等）的梯度，并更新它们。

## GPT-1

### 链接与要点

链接：[深入浅出：用中学数学理解BERT与GPT模型，下集](https://www.bilibili.com/video/BV1dp421U7pC/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)，[BERT与GPT解密脑图 ](https://www.figma.com/board/hTnJDk3dm0JDAMKekdKQzZ/BERT与GPT解密：塑造大语言模型的巨擘?node-id=0-1&p=f&t=2B42VtCRdYRPhwL1-0) 右半边。

要点：decoder-only架构，预训练与微调范式，同时进行两个任务

### decoder-only

**1. 预测时的输入：不再是单纯的 `<s\>`，而是“Prompt”**

*   **没有 Encoder** 来专门“读”输入。在推理开始时，您输入的 Task Description 和 Prompt（例如 `"翻译：apple ->"`）会被一次性喂给模型。模型**并不区分**哪部分是您给的指令，哪部分是它要生成的，所以，Prompt 本身就扮演了“历史记录”的角色，替代了经典 Decoder 那个光秃秃的 `<s\>`。

**2. 训练原理：为什么 Prompt + 后文能预测出正确答案？**

*   **训练时：**模型在海量文本中见过无数这种结构：`问题：xxxxx？ 答案：yyyyy。`模型学到的是：当上下文呈现出“问题+问号”的模式时，接下来的概率分布应该倾向于“答案”的模式。
*   **推理时：**当你给出一个 Prompt：`Translate: hello ->`，这个 Prompt 强行构建了一个**特定的上下文环境**。模型计算 $P(\text{next\_token} | \text{"Translate: hello ->"})$。由于它在训练数据里见过成千上万次类似的 `->` 后面跟着法语单词的模式，它会发现生成“bonjour”的概率最高。
*   **本质上，Prompt 就是在“诱导”模型进入某种特定的补全模式。** 你给它的上文越像某种任务的开头，它接下来的“续写”就越像在执行这个任务。

**3. 上下文长度 (Context Window)：截断会很奇怪吗？**

- **上下文包含什么？** $ \text{当前上下文} = \text{输入 Prompt} + \text{模型已经生成的所有内容} $

- **长度如何控制？** GPT 模型有一个硬性的**最大上下文长度限制**（Max Context Length），例如 GPT-3 是 2048 个 tokens。**这是由注意力矩阵的大小决定的（硬件显存限制）**。

- **满了怎么办？（截断策略）** 如果在对话或生成过程中，`Input + Output` 的总长度超过了 2048，通常采取**滑动窗口 (Sliding Window)** 策略：**扔掉最早的：** 我们会把最旧的输入（Prompt 的最开头，或者最早的几轮对话）砍掉；**保留最新的：** 确保剩下的内容 + 新生成的词 $\le$ 2048。

- **截断不良后果**：Decoder-only 模型的硬伤之一 —— “长短期记忆丢失”。假设给了一个很长的指令：“你是一个严谨的翻译家，不要使用俚语...（中间省略2000字的文章）...请翻译最后这句。”， 如果文章太长，导致最开头的“你是一个严谨的翻译家...”被截断挤出了窗口，模型就忘了你的人设要求。

### 预训练

**预训练**任务就是transformer中的“预测下一个词”，也是利用因果掩码实现并行计算，最终实现模型参数的训练，整体架构和张量变化如下：
$$
(5,1) \xrightarrow{UW_e+W_P} (5,512) \xrightarrow{\text{n * transformer block}}  (5,512)  \xrightarrow{W^T_e} (5,1e4) \xrightarrow{\text{softmax}} (5,1e4)
$$
<img src="./decoder-only.png" alt="decoder-only" style="zoom:45%;" />

- 论文里嵌入层和输出层都用$W_e$（Weight Tying）。    

  从直觉上，输入端 $W_e$ 的作用是 “将一个词映射到其语义向量空间”。输出端$W_e^T$ 的作用是 “将一个语义向量映射回词表，两个操作本质上是互逆的。    
  作用是减少参数量、提升模型性能和收敛速度

- 当反向传播发生时：

  输出层 $W_e^T$ 的第一部分梯度为 $grad_{out}$       

  Embedding 层$W_e$ 的第二部分梯度为 $grad_{in}$    

  $W_e$ 的总梯度是 $grad = grad_{out} + grad_{in}$    

  $W_{new} = W_{old} - lr * grad$

- 参数整体可以分为两类 $W_e$ 和 其他参数。$W_e$ 即词嵌入矩阵，而其他参数是更像是模型参数。

### 微调

具体详见 [BERT与GPT解密脑图 ](https://www.figma.com/board/hTnJDk3dm0JDAMKekdKQzZ/BERT与GPT解密：塑造大语言模型的巨擘?node-id=0-1&p=f&t=2B42VtCRdYRPhwL1-0) 右下角。

判别式微调 (Discriminative Fine-Tuning)：将预训练好的模型拿过来，针对具体下游任务（如情感分类、文本蕴含等），用少量有标签的数据进行二次训练。这**通常只是在模型顶部加一个简单的线性层，并对整个模型进行微小的参数调整**。具体的各种任务详见 [深入浅出：用中学数学理解BERT与GPT模型，下集](https://www.bilibili.com/video/BV1dp421U7pC/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930)

关注点在于：**同时进行两个任务**。其关键在于 反向传播。 下面这样 加和 形式，从L2回流的“梯度倍率”为1，从L1回流的“梯度倍率”为 $λ$，

```py
# 总损失
total_loss = loss_L2 + λ * loss_L1

# 反向传播
total_loss.backward()
```



## BERT

### 链接与要点

链接：[深入浅出：用中学数学理解BERT与GPT模型，上集](https://www.bilibili.com/video/BV1tm411R7Gu?spm_id_from=333.788.recommend_more_video.1&trackid=web_related_0.router-related-2206146-5nk2d.1764863189500.994&vd_source=3e32a51d0489203920eb4337d8de4c31)，[BERT与GPT解密脑图](https://www.figma.com/board/hTnJDk3dm0JDAMKekdKQzZ/BERT与GPT解密：塑造大语言模型的巨擘?node-id=0-1&p=f&t=2B42VtCRdYRPhwL1-0)

要点：训练任务与模型结构

### 预训练

预训练同时进行两个任务：预测MASK处的词 和 判断两句话是否连续

核心内容如图所示，

![bert_pretrain_1.png](.\bert_pretrain_1.png)

![bert_pretrain_2](.\bert_pretrain_2.png)

其他注意点如下：

- 两个任务同时进行，类似GPT的微调部分，

  - 样本构造：第一个位置永远是 `[CLS]`。一句 A + 一句 B，总长 ≤ 512，A 与 B 之间放 `[SEP]`，结尾再放一个 `[SEP]`。特殊符除外 15 % 的 token 被 mask。

  - 取特征：`[CLS]` 对应 hidden → 线性层 → 二分类 softmax（NSP）。所有被 mask 位置对应的 hidden → 线性层 → 词表 softmax（MLM）。

  - 损失：   $L = L_{MLM} + L_{NSP}$，相当于是倍率均为 1，对 L 做一次 `loss.backward()`，俩任务的梯度同时回传。

- mask的比例为15%，只用mask位置计算loss

- 对于矩阵（一个 $7 \times 10,000$ 的矩阵，mask个数为7）计算交叉熵损失的方式：**通常是取平均**。   

  计算过程其实是逐行进行，最后取平均。

  $$ \text{Final Loss} = \frac{\text{Loss}_1 + \text{Loss}_2 + \dots + \text{Loss}_7}{7} $$

- 考虑 batch size 此时输入维度 `(32, 7, 10000)`，仍然是平均：

  1.  先把这 $32 \times 7 = 224$ 个位置的预测全部拉平。
  2.  对这 224 个位置分别计算交叉熵。
  3.  最后把这 224 个损失值加起来除以 224，得到最终的一个 Loss 数值。

- 在后来的模型中，encoder块 或者 decoder块 采用这种方式：

<img src=".\pre_ln.png" alt="pre_ln" style="zoom:33%;" />

### 微调

具体详见 [BERT与GPT解密脑图 ](https://www.figma.com/board/hTnJDk3dm0JDAMKekdKQzZ/BERT与GPT解密：塑造大语言模型的巨擘?node-id=0-1&p=f&t=2B42VtCRdYRPhwL1-0) 左下角。

任务列表：

- 句子对分类任务 (Sentence Pair Classification Tasks)：处理两个句子并对它们关系分类。
- 单句分类任务(Single Sentence Classification Tasks)：只涉及一个句子，判断其属性，如情感分类。
- 问答任务 (Question Answering Tasks)：[CLS] + 一句问题 + [SEP] 一句段落 + [SEP]，通过两个全连接层分别预测答案的开始位置和结束位置。
- 单句标注任务(Single Sentence Tagging Tasks)：例如命名实体识别，后接11个独立全连接层识别11类。

## GPT2

### 要点与链接

链接：[GPT，GPT-2，GPT-3 论文精读](https://www.bilibili.com/video/BV1AF411b7xQ/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930) (PS: 一个多小时我全看了)

要点：zero-shot，规模效应

（概念 语言模型容量：参数量、训练数据量、训练时长（训练量））

### Zero-Shot

其关键在于 “**模型容量足够大时，能通过 Prompt 有效实现 Task Conditioning**”。论文指出，由于训练数据（互联网文本）是人类自然产生的，里面本身就包含了无数的“天然 Prompt” —— 文章中法语英语翻译的例子。

- GPT-2 认为，Zero-Shot 能力的来源，就是模型在训练数据中见过类似的“模式”。
- 当你给模型一个 Prompt（比如 `TL;DR:`），你实际上是在 唤醒 模型在海量数据中见过的某种特定上下文模式，引导它完成任务。
- 数据来源于Reddit网站(点赞>3)数据。

### 模型架构

| 模型架构       | GPT-1            | GPT-2                                                  | 作用                                   |
| -------------- | ---------------- | ------------------------------------------------------ | -------------------------------------- |
| **Layer Norm** | Post-LN (输出后) | Pre-LN (输入前)                                        | 提升深层网络训练稳定性，为更大模型铺路 |
| **初始化**     | 标准初始化       | 对残差层特殊初始化，乘以 `1/√N` 系数，N 是残差层的数量 | 同样为了深层网络训练的稳定性           |
| **上下文长度** | 512 tokens       | 1024 tokens                                            | 增强了对长文本的理解和生成能力         |
| **词汇表大小** | ~40,000          | 50,257                                                 | 覆盖更广泛的词汇                       |
| **参数量**     | 1.17 亿          | 1.17亿, 3.45亿, 7.74亿, **15亿**                       | 规模效应的直接体现                     |

## GPT3

### 要点与链接

链接：[GPT，GPT-2，GPT-3 论文精读](https://www.bilibili.com/video/BV1AF411b7xQ/?share_source=copy_web&vd_source=33a3d9253f3eaae8520547ba343f9930) (PS: 一个多小时我全看了)

要点：In-context Learning，规模效应

### 过往缺点

- 子任务上需要对大量数据进行标注
- 预训练任务数据与微调任务数据有大量重合，在预训练上过拟合后，微调任务上的高分并不能体现预训练模型有良好范化性。

### few-shot

few-shot有效来源于 In-context Learning —— 训练时每个“段落”就像一个上下文学习，他们都是相关的，比如下图中 “加法怎么做”、“错别字改正”、“英语翻译为法语”，从而获得了多种“情景再现能力”，给少量的样本相当于给定情景。

<img src=".\in-context_learning.png" alt="in-context_learning" style="zoom:33%;" />

下图可见：   
zero-shot —— task description + prompt   
one-shot —— task description + 1\*example + prompt   
few-shot —— task description + n\*example + prompt 

<img src="C:\Users\86147\Pictures\Screenshots\屏幕截图 2025-12-15 110424.png" alt="屏幕截图 2025-12-15 110424" style="zoom:40%;" />

### 模型架构

| 特性           | GPT-1   | GPT-2 (最大版) | GPT-3 (Davinci)              | 变化意义                         |
| :------------- | :------ | :------------- | :--------------------------- | :------------------------------- |
| $n_{params}$   | 1.17 亿 | 15 亿          | **1,750 亿**                 | 100倍的提升，规模效应            |
| $n_{layers}$   | 12      | 48             | **96**                       | 模型更深，推理能力更强。         |
| $d_{model}$    | 768     | 1600           | **12,288**                   | 每层记忆容量和特征表达能力提升。 |
| $d_{head}$     | 12      | 25             | **96**                       | 能够并行关注的信息维度更多。     |
| **上下文长度** | 512     | 1024           | **2048**                     | 读入更长内容，理解更长逻辑。     |
| **架构核心**   | Post-LN | **Pre-LN**     | **Pre-LN** + **Sparse Attn** | 继承  Pre-LN 保证训练稳定。      |

（数据获取 —— 如何处理Common Crawl数据：筛选了的Reddit(点赞>3)作为正例训练二分类器，然后对Common Crawl筛高质量；去重用LSH技术；再加以前的高质量数据集。）



## InstructGPT

《Training language models to follow instructions with human feedback》

### 链接与要点

要点：**RLHF**

### SFT

### RM

### PPO
